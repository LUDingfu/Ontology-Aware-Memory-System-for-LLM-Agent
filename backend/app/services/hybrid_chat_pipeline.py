"""Hybrid Pipeline for Chat Processing"""

import uuid
from typing import Any, Dict, List, Optional, Tuple
from uuid import UUID
from dataclasses import dataclass
from enum import Enum

from sqlmodel import Session, select

from app.models.memory import ChatRequest, ChatResponse, ChatEvent, Memory
from app.models.chat import PromptContext, ChatMessage
from app.services.embedding_service import EmbeddingService
from app.services.llm_service import LLMService
from app.services.retrieval_service import RetrievalService
from app.services.entity_service import EntityService
from app.services.memory_service import MemoryService
from app.services.action_knowledge_memory_classifier import ActionKnowledgeMemoryClassifier, MemoryCategory
from app.services.disambiguation_service import DisambiguationService, DisambiguationResult
from app.services.alias_mapping_service import AliasMappingService
from app.services.pii_protection_service import PIIProtectionService, PIIMatch


class ProcessingMode(Enum):
    """å¤„ç†æ¨¡å¼"""
    SIMPLE = "simple"      # ç®€åŒ–å¤„ç†ï¼šä¸€èˆ¬æ€§å¯¹è¯
    FULL = "full"          # å®Œæ•´å¤„ç†ï¼šä¸šåŠ¡ç›¸å…³å¯¹è¯


@dataclass
class PipelineContext:
    """Pipelineä¸Šä¸‹æ–‡"""
    user_id: str
    session_id: UUID
    user_message: str
    processing_mode: ProcessingMode
    entities: List[Any] = None
    query_embedding: List[float] = None
    retrieval_context: Any = None
    conversation_history: List[ChatMessage] = None
    llm_response: Any = None
    memories_to_store: List[Memory] = None
    
    # æ¶ˆæ­§ç›¸å…³å­—æ®µ
    disambiguation_needed: bool = False
    disambiguation_result: DisambiguationResult = None
    candidate_entities: List[Any] = None
    selected_entity: Any = None
    disambiguation_scores: List[float] = None
    
    # PIIä¿æŠ¤ç›¸å…³å­—æ®µ
    pii_matches: List[PIIMatch] = None


class HybridChatPipeline:
    """
    æ··åˆPipelineå®ç°
    
    æµç¨‹ï¼š
    1. å¿«é€Ÿæ„å›¾æ£€æµ‹ â†’ åˆ¤æ–­å¤„ç†æ¨¡å¼
    2. å®ä½“æå– â†’ EntityService.extract_entities()
    3. ğŸ” æ¶ˆæ­§æœåŠ¡é›†æˆ â†’ DisambiguationService
    4. ğŸ”€ æ¶ˆæ­§ç»“æœè·¯ç”± â†’ åˆ†æ”¯å†³ç­–
    5a. æ¾„æ¸…æµç¨‹ (åˆ†æ”¯A)
    5b. æ­£å¸¸æµç¨‹ (åˆ†æ”¯B)
    6. ç”ŸæˆEmbedding â†’ EmbeddingService.generate_embedding()
    7. æ£€ç´¢ä¸Šä¸‹æ–‡ â†’ RetrievalService.retrieve_context()
    8. æ„å»ºPrompt â†’ PromptContext
    9. ç”ŸæˆLLMå“åº” â†’ LLMService.generate_response()
    10. Memoryå¤„ç† â†’ åŸºäºç”¨æˆ·æŸ¥è¯¢+LLMå“åº”
    11. Memoryå­˜å‚¨ â†’ MemoryService.create_memory()
    12. å­˜å‚¨Chatäº‹ä»¶ â†’ ChatEvent
    """
    
    def __init__(self, session: Session):
        self.session = session
        self.embedding_service = EmbeddingService()
        self.llm_service = LLMService(session)
        self.retrieval_service = RetrievalService(session)
        self.entity_service = EntityService(session)
        self.memory_service = MemoryService(session)
        self.memory_classifier = ActionKnowledgeMemoryClassifier()
        
        # Disambiguation service
        self.disambiguation_service = DisambiguationService(session)
        
        # Alias mapping service
        self.alias_mapping_service = AliasMappingService(session)
        
        # PII protection service
        self.pii_protection_service = PIIProtectionService()
    
    def process(self, request: ChatRequest) -> ChatResponse:
        """Process chat request through the hybrid pipeline."""
        try:
            # Initialize pipeline context
            context = PipelineContext(
                user_id=request.user_id,
                session_id=request.session_id or uuid.uuid4(),
                user_message=request.message,
                processing_mode=ProcessingMode.FULL  # é»˜è®¤å®Œæ•´å¤„ç†
            )
            
            # Execute pipeline steps
            self._step1_quick_intent_detection(context)
            
            # PII detection
            self._step1_5_pii_detection(context)
            
            # Entity extraction
            self._step2_entity_extraction(context)
            
            # Disambiguation service integration
            self._step3_disambiguation_service_integration(context)
            
            # Branch decision
            if context.disambiguation_needed:
                return self._handle_disambiguation_flow(context)
            else:
                return self._handle_normal_flow(context)
            
        except Exception as e:
            raise Exception(f"Pipeline processing error: {str(e)}")
    
    def _step1_quick_intent_detection(self, context: PipelineContext):
        """
        æ­¥éª¤1ï¼šå¿«é€Ÿæ„å›¾æ£€æµ‹
        åˆ¤æ–­æ˜¯å¦éœ€è¦å®Œæ•´å¤„ç†è¿˜æ˜¯ç®€åŒ–å¤„ç†
        """
        print(f"DEBUG: Step 1 - Quick intent detection for: {context.user_message[:50]}...")
        
        # ä¸€èˆ¬æ€§å¯¹è¯æ£€æµ‹ï¼ˆä¼˜å…ˆçº§æ›´é«˜ï¼‰
        general_patterns = [
            'how are you', 'hello', 'hi', 'thanks', 'thank you',
            'good morning', 'good afternoon', 'good evening',
            'what is the weather', 'what time is it', 'bye', 'goodbye',
            'see you', 'take care', 'have a good day'
        ]
        
        message_lower = context.user_message.lower().strip()
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å®Œå…¨åŒ¹é…çš„ä¸€èˆ¬æ€§å¯¹è¯
        is_general_chat = any(pattern in message_lower for pattern in general_patterns)
        
        # ğŸ”¥ å¼ºåˆ¶ä¸šåŠ¡å…³é”®è¯æ£€æµ‹ - ç¡®ä¿å®¢æˆ·ä¿¡æ¯è¿›å…¥FULLæ¨¡å¼
        business_keywords = [
            'customer', 'order', 'invoice', 'payment', 'work order', 'task',
            'kai media', 'tc boiler', 'so-', 'inv-', 'wo-',
            'draft', 'send', 'reschedule', 'create', 'update', 'complete',
            'prefer', 'like', 'remember', 'policy', 'rule', 'status',
            'delivery', 'schedule', 'due', 'amount', 'balance',
            # ğŸ”¥ æ–°å¢ï¼šç¡®ä¿å®¢æˆ·ä¿¡æ¯è¿›å…¥FULLæ¨¡å¼
            'agreed', 'terms', 'net15', 'ach', 'rush', 'monthly', 'plan'
        ]
        
        has_business_content = any(keyword in message_lower for keyword in business_keywords)
        
        # ğŸ”¥ å¼ºåˆ¶FULLæ¨¡å¼ï¼šå¦‚æœåŒ…å«å®¢æˆ·ä¿¡æ¯ï¼Œå¿…é¡»ä½¿ç”¨FULLæ¨¡å¼
        customer_force_keywords = ['tc boiler', 'kai media', 'net15', 'payment terms', 'prefer', 'agreed', 'remember:']
        force_full_mode = any(keyword in message_lower for keyword in customer_force_keywords)
        
        # å†³å®šå¤„ç†æ¨¡å¼
        if force_full_mode:
            context.processing_mode = ProcessingMode.FULL
            print(f"DEBUG: Detected FULL processing mode (FORCED - customer info detected)")
        elif is_general_chat and not has_business_content:
            context.processing_mode = ProcessingMode.SIMPLE
            print(f"DEBUG: Detected SIMPLE processing mode (general chat)")
            print(f"DEBUG: is_general_chat={is_general_chat}, has_business_content={has_business_content}")
        else:
            context.processing_mode = ProcessingMode.FULL
            print(f"DEBUG: Detected FULL processing mode (business content: {has_business_content})")
            print(f"DEBUG: is_general_chat={is_general_chat}, has_business_content={has_business_content}")
    
    def _step1_5_pii_detection(self, context: PipelineContext):
        """
        æ­¥éª¤1.5ï¼šPIIæ£€æµ‹å’Œå¤„ç†
        æ£€æµ‹ç”¨æˆ·æ¶ˆæ¯ä¸­çš„ä¸ªäººèº«ä»½ä¿¡æ¯å¹¶è¿›è¡Œæ©ç å¤„ç†
        """
        print(f"DEBUG: Step 1.5 - PII detection for: {context.user_message[:50]}...")
        
        # æ£€æµ‹PII
        pii_matches = self.pii_protection_service.detect_pii(context.user_message)
        
        if pii_matches:
            print(f"DEBUG: Found {len(pii_matches)} PII matches")
            for match in pii_matches:
                print(f"DEBUG: PII match - {match.pii_type}: {match.original} -> {match.masked}")
            
            # æ©ç åŒ–ç”¨æˆ·æ¶ˆæ¯
            context.user_message = self.pii_protection_service.mask_pii(context.user_message, pii_matches)
            context.pii_matches = pii_matches
            
            print(f"DEBUG: Masked user message: {context.user_message}")
        else:
            print(f"DEBUG: No PII detected")
            context.pii_matches = []
    
    
    def _step2_entity_extraction(self, context: PipelineContext):
        """
        æ­¥éª¤2ï¼šå®ä½“æå–
        """
        print(f"DEBUG: Step 2 - Entity extraction")
        
        try:
            # æå–å®ä½“
            print(f"DEBUG: Calling EntityService.extract_entities with message: '{context.user_message}'")
            entities = self.entity_service.extract_entities(context.user_message, context.session_id, context.user_id)
            print(f"DEBUG: EntityService.extract_entities returned {len(entities)} entities")
            
            # é“¾æ¥å®ä½“åˆ°åŸŸæ•°æ®
            print(f"DEBUG: Calling EntityService.link_entities_to_domain")
            linked_entities = self.entity_service.link_entities_to_domain(entities)
            print(f"DEBUG: EntityService.link_entities_to_domain returned {len(linked_entities)} entities")
            
            # å­˜å‚¨å®ä½“åˆ°æ•°æ®åº“
            for entity in linked_entities:
                self.session.add(entity)
            self.session.commit()
            
            context.entities = linked_entities
            print(f"DEBUG: Extracted {len(linked_entities)} entities")
            
        except Exception as e:
            print(f"ERROR: Entity extraction failed: {e}")
            import traceback
            traceback.print_exc()
            context.entities = []
    
    def _step3_disambiguation_service_integration(self, context: PipelineContext):
        """æ­¥éª¤3: æ¶ˆæ­§æœåŠ¡é›†æˆï¼ˆåŒ…å«æ¾„æ¸…å¤„ç†ï¼‰"""
        print(f"DEBUG: Step 3 - Disambiguation service integration")
        
        # åŠ è½½å¯¹è¯å†å²ç”¨äºæ¾„æ¸…æ£€æµ‹
        conversation_history = self._load_conversation_history(context)
        
        # è°ƒç”¨æ¶ˆæ­§æœåŠ¡ï¼ˆåŒ…å«æ¾„æ¸…å¤„ç†ï¼‰
        disambiguation_result = self.disambiguation_service.decide_disambiguation(
            entities=context.entities,
            conversation_history=conversation_history,
            user_message=context.user_message,
            session_id=context.session_id,
            user_id=context.user_id
        )
        
        # æ›´æ–°ä¸Šä¸‹æ–‡
        context.disambiguation_needed = disambiguation_result.needed
        context.disambiguation_result = disambiguation_result
        
        if disambiguation_result.needed:
            print(f"DEBUG: Disambiguation needed for {len(disambiguation_result.candidates)} entities")
            context.candidate_entities = disambiguation_result.candidates
            context.disambiguation_scores = disambiguation_result.scores
        else:
            print(f"DEBUG: No disambiguation needed, selected: {disambiguation_result.selected}")
            context.selected_entity = disambiguation_result.selected
    
    def _load_conversation_history(self, context: PipelineContext) -> List[ChatMessage]:
        """åŠ è½½å¯¹è¯å†å²ç”¨äºæ¾„æ¸…æ£€æµ‹"""
        conversation_history = []
        try:
            chat_events = self.session.exec(
                select(ChatEvent)
                .where(ChatEvent.session_id == context.session_id)
                .order_by(ChatEvent.created_at.desc())
                .limit(10)  # Last 10 messages for context
            ).all()
            
            # Convert to ChatMessage format (reverse to get chronological order)
            for event in reversed(chat_events):
                conversation_history.append(ChatMessage(
                    role=event.role,
                    content=event.content,
                    timestamp=event.created_at
                ))
            
            print(f"DEBUG: Loaded {len(conversation_history)} messages for disambiguation")
        except Exception as e:
            print(f"Warning: Could not load conversation history for disambiguation: {e}")
            conversation_history = []
        
        return conversation_history
    
    def _store_chat_events(self, context: PipelineContext, assistant_response: str):
        """å­˜å‚¨ChatEvent"""
        print(f"DEBUG: Storing chat events")
        
        # å­˜å‚¨ç”¨æˆ·æ¶ˆæ¯
        chat_event = ChatEvent(
            session_id=context.session_id,
            role="user",
            content=context.user_message
        )
        self.session.add(chat_event)
        
        # å­˜å‚¨åŠ©æ‰‹å“åº”
        assistant_event = ChatEvent(
            session_id=context.session_id,
            role="assistant",
            content=assistant_response
        )
        self.session.add(assistant_event)
        
        # æäº¤åˆ°æ•°æ®åº“
        self.session.commit()
        print(f"DEBUG: Chat events stored successfully")
    
    def _handle_disambiguation_flow(self, context: PipelineContext) -> ChatResponse:
        """å¤„ç†æ¾„æ¸…æµç¨‹"""
        print(f"DEBUG: Handling disambiguation flow")
        
        try:
            # ç”Ÿæˆæ¾„æ¸…é—®é¢˜
            print(f"DEBUG: Building clarification prompt")
            clarification_prompt = self._build_clarification_prompt(context)
            print(f"DEBUG: Clarification prompt built: {clarification_prompt[:100]}...")
            
            # å­˜å‚¨ChatEvent
            print(f"DEBUG: Storing chat events for clarification")
            self._store_chat_events(context, clarification_prompt)
            print(f"DEBUG: Chat events stored successfully")
            
            return ChatResponse(
                reply=clarification_prompt,
                disambiguation_needed=True,
                candidate_entities=[
                    {
                        "name": entity.name,
                        "type": entity.type,
                        "external_ref": entity.external_ref
                    }
                    for entity in context.candidate_entities
                ],
                session_id=context.session_id
            )
        except Exception as e:
            print(f"ERROR: Disambiguation flow failed: {e}")
            import traceback
            traceback.print_exc()
            # è¿”å›ä¸€ä¸ªç®€å•çš„æ¾„æ¸…æç¤º
            return ChatResponse(
                reply="I found multiple possible matches. Please clarify which one you mean.",
                disambiguation_needed=True,
                candidate_entities=[],
                session_id=context.session_id
            )
    
    def _handle_normal_flow(self, context: PipelineContext) -> ChatResponse:
        """å¤„ç†æ­£å¸¸æµç¨‹"""
        print(f"DEBUG: Handling normal flow")
        
        # ç»§ç»­æ­£å¸¸Pipelineæ­¥éª¤
        self._step6_embedding_generation(context)
        self._step7_context_retrieval(context)
        self._step8_conversation_history(context)
        self._step9_prompt_building(context)
        self._step10_llm_response(context)
        self._step11_memory_processing(context)
        self._step12_memory_storage(context)
        self._step13_chat_events_storage(context)
        
        return self._build_response(context)
    
    def _build_clarification_prompt(self, context: PipelineContext) -> str:
        """æ„å»ºæ¾„æ¸…é—®é¢˜"""
        candidates = context.candidate_entities
        
        prompt = f"""I found multiple possible matches for your query. Please clarify which one you mean:

"""
        for i, entity in enumerate(candidates):
            prompt += f"{i+1}. {entity.name}\n"
        
        prompt += "\nPlease respond with the number or name of your choice."
        return prompt
    
    
    def _step6_embedding_generation(self, context: PipelineContext):
        """
        æ­¥éª¤6ï¼šç”ŸæˆEmbedding
        """
        print(f"DEBUG: Step 6 - Embedding generation")
        
        # ç”ŸæˆæŸ¥è¯¢embedding
        query_embedding = self.embedding_service.generate_embedding(context.user_message)
        if not query_embedding or len(query_embedding) == 0:
            raise Exception("Failed to generate embedding")
        
        context.query_embedding = query_embedding
        print(f"DEBUG: Generated embedding with {len(query_embedding)} dimensions")
    
    def _step7_context_retrieval(self, context: PipelineContext):
        """
        æ­¥éª¤7ï¼šæ£€ç´¢ä¸Šä¸‹æ–‡
        """
        print(f"DEBUG: Step 7 - Context retrieval")
        
        # ç®€åŒ–æ¨¡å¼ï¼šè·³è¿‡ä¸Šä¸‹æ–‡æ£€ç´¢
        if context.processing_mode == ProcessingMode.SIMPLE:
            print(f"DEBUG: Skipping context retrieval for SIMPLE mode")
            context.retrieval_context = None
            return
        
        # æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡
        retrieval_context = self.retrieval_service.retrieve_context(
            query=context.user_message,
            query_embedding=context.query_embedding,
            user_id=context.user_id,
            session_id=context.session_id
        )
        
        context.retrieval_context = retrieval_context
        print(f"DEBUG: Retrieved {len(retrieval_context.memories)} memories and {len(retrieval_context.domain_facts)} domain facts")
    
    def _step8_conversation_history(self, context: PipelineContext):
        """
        æ­¥éª¤8ï¼šåŠ è½½å¯¹è¯å†å²
        """
        print(f"DEBUG: Step 8 - Conversation history loading")
        
        conversation_history = []
        try:
            chat_events = self.session.exec(
                select(ChatEvent)
                .where(ChatEvent.session_id == context.session_id)
                .order_by(ChatEvent.created_at.desc())
                .limit(10)  # Last 10 messages for context
            ).all()
            
            # Convert to ChatMessage format (reverse to get chronological order)
            for event in reversed(chat_events):
                conversation_history.append(ChatMessage(
                    role=event.role,
                    content=event.content,
                    timestamp=event.created_at
                ))
            
            print(f"DEBUG: Loaded {len(conversation_history)} messages into conversation history")
        except Exception as e:
            print(f"Warning: Could not load conversation history: {e}")
            conversation_history = []
        
        context.conversation_history = conversation_history
    
    def _step9_prompt_building(self, context: PipelineContext):
        """
        æ­¥éª¤9ï¼šæ„å»ºPrompt
        """
        print(f"DEBUG: Step 9 - Prompt building")
        
        # æ ¹æ®å¤„ç†æ¨¡å¼é€‰æ‹©ä¸åŒçš„ç³»ç»Ÿæç¤º
        if context.processing_mode == ProcessingMode.SIMPLE:
            system_prompt = "You are a helpful assistant. Provide a brief, friendly response."
        else:
            system_prompt = "You are an intelligent business assistant with access to customer data, orders, invoices, and memory."
        
        # æ„å»ºPromptä¸Šä¸‹æ–‡
        memories = []
        domain_facts = []
        
        if context.processing_mode == ProcessingMode.FULL and context.retrieval_context:
            memories = context.retrieval_context.memories
            domain_facts = context.retrieval_context.domain_facts
        
        prompt_context = PromptContext(
            system_prompt=system_prompt,
            user_message=context.user_message,
            memories=memories,
            domain_facts=domain_facts,
            conversation_history=context.conversation_history
        )
        
        context.prompt_context = prompt_context
        print(f"DEBUG: Built prompt context with {len(prompt_context.memories)} memories")
        print(f"DEBUG: Processing mode: {context.processing_mode.value}")
        print(f"DEBUG: Retrieval context exists: {context.retrieval_context is not None}")
    
    def _step10_llm_response(self, context: PipelineContext):
        """
        æ­¥éª¤10ï¼šç”ŸæˆLLMå“åº”
        """
        print(f"DEBUG: Step 10 - LLM response generation")
        
        # ç”ŸæˆLLMå“åº”
        llm_response = self.llm_service.generate_response(context.prompt_context)
        context.llm_response = llm_response
        print(f"DEBUG: Generated LLM response: {llm_response.content[:100]}...")
    
    def _step11_memory_processing(self, context: PipelineContext):
        """
        æ­¥éª¤11ï¼šMemoryå¤„ç†
        åŸºäºç”¨æˆ·æŸ¥è¯¢+LLMå“åº”è¿›è¡ŒMemoryåˆ†ç±»å’Œæå–
        """
        print(f"DEBUG: Step 11 - Memory processing")
        
        memories_to_store = []
        
        # æ ¹æ®å¤„ç†æ¨¡å¼å†³å®šMemoryå¤„ç†ç­–ç•¥
        # ğŸ”¥ å¼ºåˆ¶æ£€æŸ¥ï¼šå¦‚æœåŒ…å«å®¢æˆ·ä¿¡æ¯ï¼Œç›´æ¥åˆ†ç±»ä¸ºsemanticï¼ˆæ— è®ºä»€ä¹ˆæ¨¡å¼ï¼‰
        customer_keywords = ["tc boiler", "kai media", "net15", "payment terms", "prefer", "agreed", "remember:"]
        if any(keyword in context.user_message.lower() for keyword in customer_keywords):
            print(f"DEBUG: Detected customer keyword, forcing semantic classification")
            memory_text = context.user_message
            memory_embedding = self.embedding_service.generate_embedding(memory_text)
            
            memory = Memory(
                text=memory_text,
                kind="semantic",  # å¼ºåˆ¶åˆ†ç±»ä¸ºsemantic
                importance=0.9,    # é«˜é‡è¦æ€§
                ttl_days=None,     # æ°¸ä¹…è®°å¿†
                embedding=memory_embedding
            )
            memories_to_store.append(memory)
            print(f"DEBUG: Created semantic memory: {memory_text[:50]}...")
        else:
            if context.processing_mode == ProcessingMode.SIMPLE:
                # ç®€åŒ–æ¨¡å¼ï¼šåˆ›å»ºçŸ­æœŸMemoryæˆ–è·³è¿‡
                if self._should_create_short_term_memory(context):
                    memory = self._create_short_term_memory(context)
                    memories_to_store.append(memory)
            else:
                # å®Œæ•´æ¨¡å¼ï¼šä½¿ç”¨ActionKnowledgeåˆ†ç±»å™¨
                memories_to_store = self._process_memories_with_classifier(context)
        
        context.memories_to_store = memories_to_store
        print(f"DEBUG: Processed {len(memories_to_store)} memories to store")
    
    def _should_create_short_term_memory(self, context: PipelineContext) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆ›å»ºçŸ­æœŸMemory"""
        # å¯¹äºä¸€èˆ¬æ€§å¯¹è¯ï¼Œåˆ›å»ºçŸ­æœŸMemoryç”¨äºä¸Šä¸‹æ–‡è¿ç»­æ€§
        return len(context.user_message) > 10  # é¿å…å­˜å‚¨å¤ªçŸ­çš„æ¶ˆæ¯
    
    def _create_short_term_memory(self, context: PipelineContext) -> Memory:
        """åˆ›å»ºçŸ­æœŸMemory"""
        memory_text = f"User said: {context.user_message}"
        memory_embedding = self.embedding_service.generate_embedding(memory_text)
        
        return Memory(
            text=memory_text,
            kind="episodic",
            importance=0.3,  # ä½é‡è¦æ€§
            ttl_days=7,      # 7å¤©è¿‡æœŸ
            embedding=memory_embedding
        )
    
    def _process_memories_with_classifier(self, context: PipelineContext) -> List[Memory]:
        """ä½¿ç”¨ActionKnowledgeåˆ†ç±»å™¨å¤„ç†Memory - åªè®°å½•ç”¨æˆ·æ“ä½œæ„å›¾ï¼Œä¸è®°å½•LLMå›å¤"""
        memories = []
        
        # ğŸ”¥ å¼ºåˆ¶æ£€æŸ¥ï¼šå¦‚æœåŒ…å«"Remember:"ï¼Œç›´æ¥åˆ†ç±»ä¸ºsemantic
        if "remember:" in context.user_message.lower():
            print(f"DEBUG: Detected 'Remember:' keyword, forcing semantic classification")
            memory_text = context.user_message
            memory_embedding = self.embedding_service.generate_embedding(memory_text)
            
            memory = Memory(
                text=memory_text,
                kind="semantic",  # å¼ºåˆ¶åˆ†ç±»ä¸ºsemantic
                importance=0.9,    # é«˜é‡è¦æ€§
                ttl_days=None,     # æ°¸ä¹…è®°å¿†
                embedding=memory_embedding
            )
            memories.append(memory)
            print(f"DEBUG: Created semantic memory: {memory_text[:50]}...")
            return memories
        
        # ğŸ”¥ å¼ºåˆ¶æ£€æŸ¥ï¼šå¦‚æœåŒ…å«å®¢æˆ·ä¿¡æ¯ï¼Œç›´æ¥åˆ†ç±»ä¸ºsemantic
        customer_keywords = ["tc boiler", "kai media", "net15", "payment terms", "prefer", "agreed"]
        if any(keyword in context.user_message.lower() for keyword in customer_keywords):
            print(f"DEBUG: Detected customer keyword, forcing semantic classification")
            memory_text = context.user_message
            memory_embedding = self.embedding_service.generate_embedding(memory_text)
            
            memory = Memory(
                text=memory_text,
                kind="semantic",  # å¼ºåˆ¶åˆ†ç±»ä¸ºsemantic
                importance=0.9,    # é«˜é‡è¦æ€§
                ttl_days=None,     # æ°¸ä¹…è®°å¿†
                embedding=memory_embedding
            )
            memories.append(memory)
            print(f"DEBUG: Created semantic memory: {memory_text[:50]}...")
            return memories
        
        # åªåˆ†æç”¨æˆ·æŸ¥è¯¢ï¼Œä¸åˆ†æLLMå“åº”
        user_memory = self.memory_classifier.classify_memory(
            text=context.user_message,
            context={
                "session_id": str(context.session_id),
                "user_id": context.user_id,
                "entities": [entity.model_dump() for entity in context.entities] if context.entities else []
            }
        )
        
        # æ ¹æ®ç”¨æˆ·æŸ¥è¯¢çš„åˆ†ç±»ç»“æœåˆ›å»ºMemory
        if user_memory.category in [MemoryCategory.ACTION, MemoryCategory.KNOWLEDGE]:
            memory_text = user_memory.text
            memory_embedding = self.embedding_service.generate_embedding(memory_text)
            
            memory = Memory(
                text=memory_text,
                kind=user_memory.kind,
                importance=user_memory.importance,
                ttl_days=user_memory.ttl_days,
                embedding=memory_embedding
            )
            memories.append(memory)
        
        # ç‰¹æ®Šå¤„ç†ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«éšå«çš„åå¥½ä¿¡æ¯
        # ä¾‹å¦‚ï¼š"Reschedule ... to Friday" å¯èƒ½éšå« "prefers Friday"
        implicit_preference = self._extract_implicit_preference(context.user_message)
        if implicit_preference:
            preference_embedding = self.embedding_service.generate_embedding(implicit_preference)
            preference_memory = Memory(
                text=implicit_preference,
                kind="semantic",
                importance=0.9,
                ttl_days=None,  # æ°¸ä¹…è®°å¿†
                embedding=preference_embedding
            )
            memories.append(preference_memory)
        
        return memories
    
    def _extract_implicit_preference(self, user_message: str) -> Optional[str]:
        """æå–éšå«çš„åå¥½ä¿¡æ¯"""
        message_lower = user_message.lower()
        
        # æ£€æŸ¥reschedule + Fridayæ¨¡å¼
        if ('reschedule' in message_lower and 'friday' in message_lower and 
            'kai media' in message_lower):
            return "Kai Media prefers Friday; align WO scheduling accordingly."
        
        # æ£€æŸ¥å…¶ä»–åå¥½æ¨¡å¼
        if ('prefer' in message_lower and 'friday' in message_lower):
            # æå–å®¢æˆ·åç§°
            if 'kai media' in message_lower:
                return "Kai Media prefers Friday deliveries for all shipments."
            elif 'tc boiler' in message_lower:
                return "TC Boiler prefers Friday deliveries for all shipments."
        
        # æ£€æŸ¥NETä»˜æ¬¾æ¡ä»¶
        if ('net' in message_lower and ('tc boiler' in message_lower or 'kai media' in message_lower)):
            if 'tc boiler' in message_lower:
                return "TC Boiler is NET15; align payment terms accordingly."
            elif 'kai media' in message_lower:
                return "Kai Media is NET15; align payment terms accordingly."
        
        return None
    
    def _step12_memory_storage(self, context: PipelineContext):
        """
        æ­¥éª¤12ï¼šMemoryå­˜å‚¨
        """
        print(f"DEBUG: Step 12 - Memory storage")
        
        # å­˜å‚¨æ‰€æœ‰Memory
        for memory in context.memories_to_store:
            self.memory_service.create_memory(
                session_id=context.session_id,
                kind=memory.kind,
                text=memory.text,
                embedding=memory.embedding,
                importance=memory.importance,
                ttl_days=memory.ttl_days,
                pii_matches=context.pii_matches  # ä¼ é€’PIIä¿¡æ¯
            )
        
        # ğŸ”¥ æ™ºèƒ½æ•´åˆMemory (åªåœ¨éœ€è¦æ—¶è§¦å‘)
        try:
            self.memory_service.consolidate_memories(user_id=context.user_id, session_window=3, force=False)
        except Exception as e:
            print(f"Warning: Memory consolidation failed: {e}")
        
        print(f"DEBUG: Stored {len(context.memories_to_store)} memories")
    
    def _step13_chat_events_storage(self, context: PipelineContext):
        """
        æ­¥éª¤13ï¼šå­˜å‚¨Chatäº‹ä»¶
        """
        print(f"DEBUG: Step 13 - Chat events storage")
        
        # å­˜å‚¨ç”¨æˆ·æ¶ˆæ¯
        chat_event = ChatEvent(
            session_id=context.session_id,
            role="user",
            content=context.user_message
        )
        self.session.add(chat_event)
        
        # å­˜å‚¨åŠ©æ‰‹å“åº”
        assistant_event = ChatEvent(
            session_id=context.session_id,
            role="assistant",
            content=context.llm_response.content
        )
        self.session.add(assistant_event)
        self.session.commit()
        
        print(f"DEBUG: Stored chat events")
    
    def _build_response(self, context: PipelineContext) -> ChatResponse:
        """æ„å»ºå“åº”"""
        print(f"DEBUG: Building response - Processing mode: {context.processing_mode.value}")
        print(f"DEBUG: Retrieval context exists: {context.retrieval_context is not None}")
        if context.retrieval_context:
            print(f"DEBUG: Retrieval context memories: {len(context.retrieval_context.memories)}")
            print(f"DEBUG: Retrieval context domain facts: {len(context.retrieval_context.domain_facts)}")
        
        # æ ¼å¼åŒ–ä½¿ç”¨çš„è®°å¿†
        used_memories = []
        if (context.processing_mode == ProcessingMode.FULL and 
            context.retrieval_context and 
            context.retrieval_context.memories):
            used_memories = [
                {
                    "memory_id": memory.memory_id,
                    "text": memory.text,
                    "similarity": memory.similarity,
                    "kind": memory.kind
                }
                for memory in context.retrieval_context.memories
            ]
        
        # æ ¼å¼åŒ–ä½¿ç”¨çš„åŸŸäº‹å®
        used_domain_facts = []
        if (context.processing_mode == ProcessingMode.FULL and 
            context.retrieval_context and 
            context.retrieval_context.domain_facts):
            used_domain_facts = [
                {
                    "table": fact.table,
                    "id": fact.id,
                    "data": fact.data,
                    "relevance_score": fact.relevance_score
                }
                for fact in context.retrieval_context.domain_facts
            ]
        
        # æ ¼å¼åŒ–å€™é€‰å®ä½“
        candidate_entities = []
        if context.candidate_entities:
            candidate_entities = [
                {
                    "name": entity.name,
                    "type": entity.type,
                    "external_ref": entity.external_ref
                }
                for entity in context.candidate_entities
            ]
        
        return ChatResponse(
            reply=context.llm_response.content,
            used_memories=used_memories,
            used_domain_facts=used_domain_facts,
            session_id=context.session_id,
            disambiguation_needed=context.disambiguation_needed,
            candidate_entities=candidate_entities
        )
    
    def _handle_normal_flow_with_selected_entity(self, context: PipelineContext) -> ChatResponse:
        """å¤„ç†å¸¦æœ‰å·²é€‰æ‹©å®ä½“çš„æ­£å¸¸æµç¨‹"""
        print(f"DEBUG: Handling normal flow with selected entity: {context.selected_entity}")
        
        # è·³è¿‡å®ä½“æå–ï¼Œç›´æ¥è¿›è¡Œåç»­æ­¥éª¤
        self._step6_embedding_generation(context)
        self._step7_context_retrieval(context)
        self._step8_conversation_history_loading(context)
        self._step9_prompt_building(context)
        self._step10_llm_response_generation(context)
        self._step11_memory_processing(context)
        self._step12_memory_storage(context)
        self._step13_chat_events_storage(context)
        
        return self._build_response(context)

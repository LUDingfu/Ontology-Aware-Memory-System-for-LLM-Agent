"""Memory management service."""

import hashlib
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional
from uuid import UUID

from sqlmodel import Session, select, func

from app.models.memory import Memory, MemorySummary
from app.models.chat import MemoryRetrievalResult
from app.services.pii_protection_service import PIIMatch


class MemoryService:
    """Service for managing LLM agent memories."""
    
    def __init__(self, session: Session):
        self.session = session
    
    def create_memory(
        self,
        session_id: UUID,
        kind: str,
        text: str,
        embedding: Optional[List[float]] = None,
        importance: float = 0.5,
        ttl_days: Optional[int] = None,
        pii_matches: Optional[List[PIIMatch]] = None
    ) -> Memory:
        """Create a new memory with improved deduplication and PII protection."""
        
        # å¦‚æœæœ‰PIIï¼Œä½¿ç”¨æ©ç ç‰ˆæœ¬å­˜å‚¨
        if pii_matches:
            from app.services.pii_protection_service import PIIProtectionService
            pii_service = PIIProtectionService()
            masked_text = pii_service.create_masked_memory_text(text, pii_matches)
            print(f"DEBUG: Storing masked memory: {masked_text}")
            text = masked_text
        
        # Check for exact duplicate in same session
        existing = self.session.exec(
            select(Memory).where(
                Memory.text == text,
                Memory.session_id == session_id
            )
        ).first()
        
        if existing:
            # Update importance if new one is higher
            if importance > existing.importance:
                existing.importance = importance
                self.session.commit()
                self.session.refresh(existing)
            return existing
        
        # Check for similar memories across all sessions (for semantic memories)
        if kind == "semantic":
            similar_memories = self.session.exec(
                select(Memory).where(
                    Memory.kind == "semantic",
                    Memory.text.contains(text[:50])  # Check if similar text exists
                )
            ).all()
            
            for similar in similar_memories:
                if self._is_similar_memory(text, similar.text):
                    # Update existing memory with higher importance
                    if importance > similar.importance:
                        similar.importance = importance
                        self.session.commit()
                        self.session.refresh(similar)
                    return similar
        
        # Create new memory
        memory = Memory(
            session_id=session_id,
            kind=kind,
            text=text,
            embedding=embedding,
            importance=importance,
            ttl_days=ttl_days
        )
        
        self.session.add(memory)
        self.session.commit()
        self.session.refresh(memory)
        
        return memory
    
    def _is_similar_memory(self, text1: str, text2: str) -> bool:
        """Check if two memory texts are similar enough to be considered duplicates."""
        # Simple similarity check - can be improved with more sophisticated algorithms
        text1_clean = text1.lower().strip()
        text2_clean = text2.lower().strip()
        
        # Exact match
        if text1_clean == text2_clean:
            return True
        
        # Check if one contains the other (for partial matches)
        if len(text1_clean) > 20 and len(text2_clean) > 20:
            if text1_clean in text2_clean or text2_clean in text1_clean:
                return True
        
        # Check for high word overlap (simple approach)
        words1 = set(text1_clean.split())
        words2 = set(text2_clean.split())
        
        if len(words1) > 0 and len(words2) > 0:
            overlap = len(words1.intersection(words2))
            total_words = len(words1.union(words2))
            similarity = overlap / total_words if total_words > 0 else 0
            
            # Consider similar if > 80% word overlap
            return similarity > 0.8
        
        return False
    
    def retrieve_memories(
        self,
        query_embedding: List[float],
        user_id: str,
        session_id: Optional[UUID] = None,
        kind: Optional[str] = None,
        limit: int = 10
    ) -> List[MemoryRetrievalResult]:
        """Retrieve relevant memories using vector similarity."""
        # Build query - ä¿®å¤ï¼šç§»é™¤session_idé™åˆ¶ä»¥å…è®¸è·¨ä¼šè¯æ£€ç´¢
        query = select(Memory)
        
        # æ³¨é‡Šæ‰session_idé™åˆ¶ï¼Œå…è®¸è·¨ä¼šè¯æ£€ç´¢è®°å¿†
        # if session_id:
        #     query = query.where(Memory.session_id == session_id)
        
        if kind:
            query = query.where(Memory.kind == kind)
        
        # Filter expired memories (simplified - TTL functionality can be added later)
        # For now, we'll skip TTL filtering to avoid SQL complexity
        # query = query.where(
        #     (Memory.ttl_days.is_(None)) |
        #     (Memory.created_at + func.make_interval(days=Memory.ttl_days) > now)
        # )
        
        # Execute query and calculate similarities
        memories = self.session.exec(query).all()
        
        results = []
        for memory in memories:
            if memory.embedding is not None:
                try:
                    # Convert pgvector to list if needed
                    embedding_list = list(memory.embedding) if hasattr(memory.embedding, '__iter__') else memory.embedding
                    if len(embedding_list) > 0:
                        # Calculate cosine similarity
                        similarity = self._cosine_similarity(query_embedding, embedding_list)
                        
                        # Weight by importance and recency
                        recency_weight = self._calculate_recency_weight(memory.created_at)
                        final_score = similarity * memory.importance * recency_weight
                        
                        results.append(MemoryRetrievalResult(
                            memory_id=memory.memory_id,
                            text=memory.text,
                            kind=memory.kind,
                            similarity=final_score,
                            importance=memory.importance,
                            created_at=memory.created_at
                        ))
                except Exception as e:
                    print(f"Error processing memory {memory.memory_id}: {e}")
                    continue
        
        # Sort by similarity score and return top results
        results.sort(key=lambda x: x.similarity, reverse=True)
        return results[:limit]
    
    def consolidate_memories(
        self,
        user_id: str,
        session_window: int = 3,
        force: bool = False
    ) -> MemorySummary:
        """æ™ºèƒ½æ•´åˆè®°å¿† - åªåœ¨ç‰¹å®šæ¡ä»¶ä¸‹è§¦å‘"""
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘æ‘˜è¦ç”Ÿæˆ
        if not force and not self._should_trigger_consolidation(user_id):
            print("DEBUG: Skipping consolidation - no trigger conditions met")
            return None
        
        # Get recent memories
        cutoff_date = datetime.utcnow() - timedelta(days=30)  # Last 30 days
        memories = self.session.exec(
            select(Memory).where(Memory.created_at >= cutoff_date)
        ).all()
        
        if not memories:
            return None
        
        # Process episodic memories for potential conversion to semantic
        self._process_episodic_memories(memories)
        
        # ç”Ÿæˆæ™ºèƒ½æ‘˜è¦
        summary_text = self._generate_smart_summary(user_id, memories)
        
        # Create or update summary
        existing_summary = self.session.exec(
            select(MemorySummary).where(
                MemorySummary.user_id == user_id,
                MemorySummary.session_window == session_window
            )
        ).first()
        
        if existing_summary:
            existing_summary.summary = summary_text
            existing_summary.created_at = datetime.utcnow()
            self.session.commit()
            self.session.refresh(existing_summary)
            return existing_summary
        else:
            summary = MemorySummary(
                user_id=user_id,
                session_window=session_window,
                summary=summary_text
            )
            self.session.add(summary)
            self.session.commit()
            self.session.refresh(summary)
            return summary
    
    def _should_trigger_consolidation(self, user_id: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘æ‘˜è¦ç”Ÿæˆ"""
        # è·å–æœ€è¿‘30å¤©çš„è®°å¿†
        cutoff_date = datetime.utcnow() - timedelta(days=30)
        recent_memories = self.session.exec(
            select(Memory).where(Memory.created_at >= cutoff_date)
        ).all()
        
        if not recent_memories:
            return False
        
        # ğŸ”¥ å¼ºåˆ¶è§¦å‘æ¡ä»¶ï¼šå¦‚æœåŒ…å«ç‰¹å®šå®¢æˆ·ä¿¡æ¯
        customer_keywords = ["tc boiler", "kai media", "net15", "payment plan", "rush work order"]
        for memory in recent_memories:
            memory_text_lower = memory.text.lower()
            if any(keyword in memory_text_lower for keyword in customer_keywords):
                print(f"DEBUG: Trigger consolidation - customer keyword detected: {memory.text[:50]}...")
                return True
        
        # æ£€æŸ¥è§¦å‘æ¡ä»¶
        for memory in recent_memories:
            # æ¡ä»¶1: è¿‡æ—¶åå¥½éœ€è¦ç¡®è®¤ (Case 10)
            if self._is_stale_preference(memory):
                print(f"DEBUG: Trigger consolidation - stale preference: {memory.text[:50]}...")
                return True
            
            # æ¡ä»¶2: åŒä¸€å®¢æˆ·ç§¯ç´¯è¶³å¤Ÿä¿¡æ¯ (Case 14)
            if self._has_sufficient_customer_context(memory):
                print(f"DEBUG: Trigger consolidation - sufficient customer context: {memory.text[:50]}...")
                return True
            
            # æ¡ä»¶3: ä»»åŠ¡å®Œæˆæˆ–é‡è¦äº‹ä»¶ (Case 18)
            if self._is_task_completion(memory):
                print(f"DEBUG: Trigger consolidation - task completion: {memory.text[:50]}...")
                return True
        
        return False
    
    def _is_stale_preference(self, memory: Memory) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºè¿‡æ—¶åå¥½ (Case 10)"""
        if memory.kind != "semantic":
            return False
        
        # æ£€æŸ¥åå¥½å¹´é¾„å’Œå¼ºåŒ–åº¦
        age_days = (datetime.utcnow() - memory.created_at).days
        text_lower = memory.text.lower()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«åå¥½å…³é”®è¯
        preference_keywords = ["prefer", "like", "delivery", "payment", "terms", "remember"]
        is_preference = any(keyword in text_lower for keyword in preference_keywords)
        
        # ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦ä¸ºè¿‡æ—¶åå¥½ï¼ˆå¹´é¾„>90å¤©æˆ–é‡è¦æ€§<0.7ï¼‰
        return is_preference and (age_days > 90 or memory.importance < 0.7)
    
    def _has_sufficient_customer_context(self, memory: Memory) -> bool:
        """æ£€æŸ¥æ˜¯å¦ç§¯ç´¯äº†è¶³å¤Ÿçš„å®¢æˆ·ä¸Šä¸‹æ–‡ (Case 14)"""
        customer = self._extract_customer_from_memory(memory)
        if not customer:
            return False
        
        # æ£€æŸ¥è¯¥å®¢æˆ·æœ€è¿‘30å¤©çš„è®°å¿†æ•°é‡
        cutoff_date = datetime.utcnow() - timedelta(days=30)
        customer_memories = self.session.exec(
            select(Memory).where(
                Memory.created_at >= cutoff_date,
                Memory.text.contains(customer)
            )
        ).all()
        
        # é™ä½é˜ˆå€¼ï¼Œæ›´å®¹æ˜“è§¦å‘
        return len(customer_memories) >= 3  # é˜ˆå€¼é™ä½åˆ°3
    
    def _is_task_completion(self, memory: Memory) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºä»»åŠ¡å®Œæˆ (Case 18)"""
        text_lower = memory.text.lower()
        completion_keywords = ["completed", "done", "finished", "resolved", "closed", "marked as done"]
        return any(keyword in text_lower for keyword in completion_keywords)
    
    def _extract_customer_from_memory(self, memory: Memory) -> Optional[str]:
        """ä»è®°å¿†ä¸­æå–å®¢æˆ·åç§°"""
        text_lower = memory.text.lower()
        
        # æ‰©å±•å·²çŸ¥å®¢æˆ·åˆ—è¡¨ï¼ŒåŒ…å«æ›´å¤šå˜ä½“
        known_customers = [
            "kai media", "tc boiler", "john gai media",
            "kai media", "tc boiler", "john gai media",
            "kai", "tc", "john"
        ]
        
        # æ›´çµæ´»çš„åŒ¹é…é€»è¾‘
        for customer in known_customers:
            if customer in text_lower:
                # è¿”å›æ ‡å‡†åŒ–çš„å®¢æˆ·åç§°
                if customer in ["kai", "kai media"]:
                    return "kai media"
                elif customer in ["tc", "tc boiler"]:
                    return "tc boiler"
                elif customer in ["john", "john gai media"]:
                    return "john gai media"
                else:
                    return customer
        
        return None
    
    def _generate_smart_summary(self, user_id: str, memories: List[Memory]) -> str:
        """ç”Ÿæˆæ™ºèƒ½æ‘˜è¦"""
        print(f"DEBUG: Generating smart summary for {len(memories)} memories")
        
        # æŒ‰å®¢æˆ·åˆ†ç»„
        customer_groups = {}
        for memory in memories:
            customer = self._extract_customer_from_memory(memory)
            if customer:
                if customer not in customer_groups:
                    customer_groups[customer] = []
                customer_groups[customer].append(memory)
                print(f"DEBUG: Added memory to {customer} group: {memory.text[:50]}...")
        
        print(f"DEBUG: Found {len(customer_groups)} customer groups: {list(customer_groups.keys())}")
        
        if not customer_groups:
            # æ²¡æœ‰å®¢æˆ·ä¿¡æ¯ï¼Œç”Ÿæˆé€šç”¨æ‘˜è¦
            return f"Memory consolidation for user {user_id}: {len(memories)} memories processed"
        
        # ä¸ºæ¯ä¸ªå®¢æˆ·ç”Ÿæˆæ‘˜è¦
        summary_parts = []
        for customer, customer_memories in customer_groups.items():
            print(f"DEBUG: Processing {customer} with {len(customer_memories)} memories")
            customer_summary = self._extract_customer_key_info(customer, customer_memories)
            if customer_summary:
                summary_parts.append(f"{customer.title()}: {customer_summary}")
                print(f"DEBUG: Generated summary for {customer}: {customer_summary}")
            else:
                print(f"DEBUG: No summary generated for {customer}")
        
        if summary_parts:
            result = f"Customer Summary ({len(customer_groups)} customers): " + "; ".join(summary_parts)
            print(f"DEBUG: Final summary: {result}")
            return result
        else:
            return f"Memory consolidation for user {user_id}: {len(memories)} memories processed"
    
    def _extract_customer_key_info(self, customer: str, memories: List[Memory]) -> str:
        """æå–å®¢æˆ·å…³é”®ä¿¡æ¯ - Rule-basedæ–¹æ³•"""
        print(f"DEBUG: Extracting key info for {customer} from {len(memories)} memories")
        
        terms = []
        orders = []
        payments = []
        preferences = []
        
        for memory in memories:
            text_lower = memory.text.lower()
            print(f"DEBUG: Processing memory: {memory.text[:50]}...")
            
            # æå–æ¡æ¬¾ä¿¡æ¯
            if any(keyword in text_lower for keyword in ["net", "terms", "payment", "agreed"]):
                terms.append(memory.text)
                print(f"DEBUG: Found terms: {memory.text}")
            
            # æå–è®¢å•ä¿¡æ¯
            if any(keyword in text_lower for keyword in ["so-", "work order", "wo-", "rush"]):
                orders.append(memory.text)
                print(f"DEBUG: Found orders: {memory.text}")
            
            # æå–ä»˜æ¬¾ä¿¡æ¯
            if any(keyword in text_lower for keyword in ["payment plan", "monthly", "$", "pay", "500"]):
                payments.append(memory.text)
                print(f"DEBUG: Found payments: {memory.text}")
            
            # æå–åå¥½ä¿¡æ¯
            if any(keyword in text_lower for keyword in ["prefer", "like", "delivery", "friday", "thursday", "ach"]):
                preferences.append(memory.text)
                print(f"DEBUG: Found preferences: {memory.text}")
        
        # æ„å»ºå®¢æˆ·æ‘˜è¦
        customer_info = []
        if terms:
            # æå–NETæ¡æ¬¾
            for term in terms:
                if "net15" in term.lower():
                    customer_info.append("Terms: NET15")
                    break
                elif "net" in term.lower():
                    customer_info.append(f"Terms: {term}")
        
        if orders:
            # æå–è®¢å•ä¿¡æ¯
            for order in orders:
                if "so-2002" in order.lower():
                    customer_info.append("Orders: Rush WO for SO-2002")
                    break
                elif "so-" in order.lower():
                    customer_info.append(f"Orders: {order}")
        
        if payments:
            # æå–ä»˜æ¬¾ä¿¡æ¯
            for payment in payments:
                if "500" in payment.lower():
                    customer_info.append("Payments: $500/month plan")
                    break
                elif "payment plan" in payment.lower():
                    customer_info.append(f"Payments: {payment}")
        
        if preferences:
            # æå–åå¥½ä¿¡æ¯
            for pref in preferences:
                if "ach" in pref.lower():
                    customer_info.append("Preferences: ACH payments")
                    break
                elif "friday" in pref.lower():
                    customer_info.append("Preferences: Friday delivery")
                    break
        
        result = "; ".join(customer_info) if customer_info else ""
        print(f"DEBUG: Generated customer info for {customer}: {result}")
        return result
    
    def _process_episodic_memories(self, memories: List[Memory]) -> None:
        """Process episodic memories and convert patterns to semantic memories."""
        episodic_memories = [m for m in memories if m.kind == "episodic"]
        
        # Look for patterns in episodic memories that should become semantic
        for memory in episodic_memories:
            text = memory.text.lower()
            
            # Pattern: "X prefers Y" or "X likes Y" from multiple episodes
            if any(word in text for word in ["prefers", "likes", "dislikes", "always", "never"]):
                # Check if we have similar patterns from other episodes
                similar_episodic = [
                    m for m in episodic_memories 
                    if m.memory_id != memory.memory_id and 
                    any(word in m.text.lower() for word in ["prefers", "likes", "dislikes", "always", "never"])
                ]
                
                if len(similar_episodic) >= 1:  # If we have at least 2 similar patterns
                    # Extract the semantic part (remove time-specific words)
                    semantic_text = self._extract_semantic_from_episodic(memory.text)
                    if semantic_text:
                        # Create semantic memory
                        self.create_memory(
                            session_id=memory.session_id,
                            kind="semantic",
                            text=semantic_text,
                            importance=0.9,  # High importance for consolidated patterns
                            ttl_days=None  # Permanent
                        )
    
    def _extract_semantic_from_episodic(self, episodic_text: str) -> Optional[str]:
        """Extract semantic meaning from episodic memory text."""
        text = episodic_text.lower()
        
        # Remove time-specific words
        time_words = ["today", "yesterday", "just", "recently", "now", "sent", "drafted", "created"]
        for word in time_words:
            text = text.replace(word, "")
        
        # Look for preference patterns
        if "prefers" in text:
            # Extract the preference part
            parts = text.split("prefers")
            if len(parts) > 1:
                return f"Customer prefers {parts[1].strip()}"
        
        if "likes" in text:
            parts = text.split("likes")
            if len(parts) > 1:
                return f"Customer likes {parts[1].strip()}"
        
        if "dislikes" in text:
            parts = text.split("dislikes")
            if len(parts) > 1:
                return f"Customer dislikes {parts[1].strip()}"
        
        return None
    
    def get_user_memories(
        self,
        user_id: str,
        limit: int = 10
    ) -> List[Memory]:
        """Get all memories for a user."""
        # This would need to be implemented based on how user_id relates to sessions
        # For now, return recent memories
        memories = self.session.exec(
            select(Memory).order_by(Memory.created_at.desc()).limit(limit)
        ).all()
        
        return memories
    
    def _cosine_similarity(self, a: List[float], b: List[float]) -> float:
        """Calculate cosine similarity between two vectors."""
        import math
        
        dot_product = sum(x * y for x, y in zip(a, b))
        magnitude_a = math.sqrt(sum(x * x for x in a))
        magnitude_b = math.sqrt(sum(x * x for x in b))
        
        if magnitude_a == 0 or magnitude_b == 0:
            return 0.0
        
        return dot_product / (magnitude_a * magnitude_b)
    
    def _calculate_recency_weight(self, created_at: datetime) -> float:
        """Calculate recency weight for memory."""
        from datetime import timezone
        # Make sure both datetimes are timezone-aware
        now = datetime.now(timezone.utc)
        if created_at.tzinfo is None:
            created_at = created_at.replace(tzinfo=timezone.utc)
        days_old = (now - created_at).days
        return max(0.1, 1.0 - (days_old / 365.0))  # Decay over a year
